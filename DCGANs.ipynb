{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MKzWFPjBE3W"
      },
      "outputs": [],
      "source": [
        "!wget https://www.dropbox.com/s/rbajpdlh7efkdo1/male_female_face_images.zip\n",
        "!unzip /content/male_female_face_images.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch_snippets"
      ],
      "metadata": {
        "id": "0QekXs1JELNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_snippets import *\n",
        "import torchvision\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchvision.utils as vutils \n",
        "import cv2, numpy as np, pandas as pd\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "face_cascade=cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "!mkdir cropped_faces\n",
        "images = Glob('/content/females/*.jpg') + Glob('/content/males/*.jpg')\n",
        "\n",
        "for i in range(len(images)):\n",
        "  img = read(images[i],1)\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "  for (x,y,w,h) in faces:\n",
        "    imgs = img[y:(y+h),x:(x+w),:]\n",
        "  cv2.imwrite('/cropped_faces/' + str(i) + '.jpg', cv2.cvtColor(imgs, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize(64), transforms.CenterCrop(64),transforms.ToTensor(), transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])])"
      ],
      "metadata": {
        "id": "BS1EuSinB_Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FacesData(Dataset):\n",
        "  def __init__(self,folder):\n",
        "    super().__init__()\n",
        "    self.folder = folder\n",
        "    self.images = sorted(Glob(folder))\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    image_path = self.images[index]\n",
        "    image = Image.open(image_path)\n",
        "    image = transform(image)\n",
        "    return image\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = nn.Sequential(\n",
        "                    nn.Conv2d(3,64,4,2,1,bias=False),\n",
        "                    nn.LeakyReLU(0.2,inplace=True),\n",
        "                    nn.Conv2d(64,64*2,4,2,1,bias=False),\n",
        "                    nn.BatchNorm2d(64*2),\n",
        "                    nn.LeakyReLU(0.2,inplace=True),\n",
        "                    nn.Conv2d(64*2,64*4,4,2,1,bias=False),\n",
        "                    nn.BatchNorm2d(64*4),\n",
        "                    nn.LeakyReLU(0.2,inplace=True),\n",
        "                    nn.Conv2d(64*4,64*8,4,2,1,bias=False),\n",
        "                    nn.BatchNorm2d(64*8),\n",
        "                    nn.LeakyReLU(0.2,inplace=True),\n",
        "                    nn.Conv2d(64*8,1,4,1,0,bias=False),\n",
        "                    nn.Sigmoid()\n",
        "                )\n",
        "  def forward(self, input):\n",
        "    return self.model(input)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator,self).__init__()\n",
        "    self.model = nn.Sequential(\n",
        "        nn.ConvTranspose2d(100,64*8,4,1,0,bias=False,),\n",
        "        nn.BatchNorm2d(64*8),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(64*8,64*4,4,2,1,bias=False),\n",
        "        nn.BatchNorm2d(64*4),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d( 64*4,64*2,4,2,1,bias=False),\n",
        "        nn.BatchNorm2d(64*2),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d( 64*2,64,4,2,1,bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d( 64,3,4,2,1,bias=False),\n",
        "        nn.Tanh())\n",
        "  \n",
        "  def forward(self,input): \n",
        "    return self.model(input)"
      ],
      "metadata": {
        "id": "5LryysrsDjcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator_train_step(real_data, fake_data):\n",
        "  d_optimizer.zero_grad()\n",
        "  prediction_real = discriminator(real_data)\n",
        "  error_real = loss(prediction_real.squeeze(), \\\n",
        "                    torch.ones(len(real_data)).to(device))\n",
        "  error_real.backward()\n",
        "  prediction_fake = discriminator(fake_data)\n",
        "  error_fake = loss(prediction_fake.squeeze(), \\\n",
        "                    torch.zeros(len(fake_data)).to(device))\n",
        "  error_fake.backward()\n",
        "  d_optimizer.step()\n",
        "  return error_real + error_fake\n",
        "\n",
        "def generator_train_step(fake_data):\n",
        "    g_optimizer.zero_grad()\n",
        "    prediction = discriminator(fake_data)\n",
        "    error = loss(prediction.squeeze(), \\\n",
        "                torch.ones(len(real_data)).to(device))\n",
        "    error.backward()\n",
        "    g_optimizer.step()\n",
        "    return error\n",
        "\n",
        "discriminator = Discriminator().to(device)\n",
        "generator = Generator().to(device)\n",
        "loss = nn.BCELoss()\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "\n",
        "log = Report(25)\n",
        "for epoch in range(25):\n",
        "  N = len(dataloader)\n",
        "  for i, images in enumerate(tqdm(dataloader)):\n",
        "    real_data = images.to(device)\n",
        "    fake_data = generator(torch.randn(len(real_data), 100, 1, 1).to(device)).to(device)\n",
        "    fake_data = fake_data.detach()\n",
        "    d_loss=discriminator_train_step(real_data, fake_data)\n",
        "    fake_data = generator(torch.randn(len(real_data), 100, 1, 1).to(device)).to(device)\n",
        "    g_loss = generator_train_step(fake_data)\n",
        "    log.record(epoch+(1+i)/N, d_loss=d_loss.item(), g_loss=g_loss.item(), end='\\r')\n",
        "    log.report_avgs(epoch+1)\n",
        "log.plot_epochs(['d_loss','g_loss'])\n",
        "\n",
        "generator.eval()\n",
        "noise = torch.randn(64, 100, 1, 1, device=device)\n",
        "sample_images = generator(noise).detach().cpu()\n",
        "grid = vutils.make_grid(sample_images,nrow=8,normalize=True)\n",
        "show(grid.cpu().detach().permute(1,2,0), sz=10, title='Generated images')"
      ],
      "metadata": {
        "id": "FU5F3r7FF-Ns"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}