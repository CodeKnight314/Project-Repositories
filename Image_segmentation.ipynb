{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_snippets"
      ],
      "metadata": {
        "id": "A9AeTXQC1iPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_snippets import *\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.optim import Adam, lr_scheduler\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Fc6A8lgLPsPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegData(Dataset):\n",
        "  def __init__(self, image_directory, annotation_directory):\n",
        "    self.items = stems(image_directory)\n",
        "    self.image_directory = image_directory\n",
        "    self.annotation_directory = annotation_directory\n",
        "    self.transformer = transforms.Compose([transforms.ToTensor(), \n",
        "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.items)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    image = read(f'{self.image_directory}/{self.items[index]}.png', 1)\n",
        "    image = cv2.resize(image, (224,224))\n",
        "    mask = read(f'{self.annotation_directory}/{self.items[index]}.png')\n",
        "    mask = cv2.resize(mask, (224,224))\n",
        "    return image, mask\n",
        "\n",
        "  def collate_fn(self, batch):\n",
        "    ims, masks = list(zip(*batch))\n",
        "    ims = torch.cat([self.transformer(im.copy()/255.)[None] for im in ims]).float().to(device)\n",
        "    ce_masks = torch.cat([torch.Tensor(mask[None]) for mask in masks]).long().to(device)\n",
        "    return ims, ce_masks"
      ],
      "metadata": {
        "id": "O6FgGBlnzW2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = self.contract_block(in_channels, 32, 7, 3)\n",
        "        self.conv2 = self.contract_block(32, 64, 3, 1)\n",
        "        self.conv3 = self.contract_block(64, 128, 3, 1)\n",
        "\n",
        "        self.upconv3 = self.expand_block(128, 64, 3, 1)\n",
        "        self.upconv2 = self.expand_block(64*2, 32, 3, 1)\n",
        "        self.upconv1 = self.expand_block(32*2, out_channels, 3, 1)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        conv1 = self.conv1(x)\n",
        "        conv2 = self.conv2(conv1)\n",
        "        conv3 = self.conv3(conv2)\n",
        "\n",
        "        upconv3 = self.upconv3(conv3)\n",
        "\n",
        "        upconv2 = self.upconv2(torch.cat([upconv3, conv2], 1))\n",
        "        upconv1 = self.upconv1(torch.cat([upconv2, conv1], 1))\n",
        "\n",
        "        return upconv1\n",
        "\n",
        "    def contract_block(self, in_channels, out_channels, kernel_size, padding):\n",
        "\n",
        "        contract = nn.Sequential(\n",
        "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
        "            torch.nn.BatchNorm2d(out_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
        "            torch.nn.BatchNorm2d(out_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "                                 )\n",
        "\n",
        "        return contract\n",
        "\n",
        "    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n",
        "\n",
        "        expand = nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
        "                            torch.nn.BatchNorm2d(out_channels),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
        "                            torch.nn.BatchNorm2d(out_channels),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1) \n",
        "                            )\n",
        "        return expand"
      ],
      "metadata": {
        "id": "xmo48QB-4IIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(prediction, target):\n",
        "  ce = nn.CrossEntropyLoss()\n",
        "  ce_loss = ce(prediction, target)\n",
        "  acc = (torch.max(prediction, 1)[1] == target).float().mean()\n",
        "  return ce_loss, acc\n",
        "\n",
        "def train_batch(model, data, optimizer):\n",
        "  model.train()\n",
        "  ims, ce_mask = data \n",
        "  prediction = model(ims)\n",
        "  optimizer.zero_grad()\n",
        "  loss_value, acc = loss_function(prediction, ce_mask)\n",
        "  loss_value.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  return loss_value.item(), acc.item()\n",
        "\n",
        "def valid_batch(model, data):\n",
        "  model.eval()\n",
        "  ims, ce_mask = data\n",
        "  prediction = model(ims)\n",
        "  loss_value, acc = loss_function(prediction, ce_mask)\n",
        "  return loss_value.item(), acc.item()\n",
        "\n",
        "def get_model():\n",
        "  model = UNET(3,12).to(device)\n",
        "  optimizer = Adam(model.parameters(), lr = 1e-3)\n",
        "  return model, optimizer\n",
        "\n",
        "def save_plot(tr_list, val_list, title):\n",
        "  plt.plot(tr_list, label = \"Training\")\n",
        "  plt.plot(val_list, label = \"Validation\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.legend()\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "  plt.savefig(f\"{title}.png\")\n",
        "  print(f\"Saved {title}.png\")\n",
        "\n",
        "def get_dataLoaders(image_directory, annotation_directory):\n",
        "  ds_set = SegData(image_directory, annotation_directory)\n",
        "  ds_loader = DataLoader(ds_set, batch_size = 4, shuffle = True, collate_fn = ds_set.collate_fn)\n",
        "  return ds_loader\n",
        "\n",
        "def run(epoch_range, train_dir, train_anno_dir, val_dir, val_anno_dir, real_dir = None, model_dict = None):\n",
        "  model, optimizer = get_model()\n",
        "  if model_dict:\n",
        "    model.load_state_dict(torch.load(model_dict))\n",
        "\n",
        "  tr_dl = get_dataLoaders(train_dir, train_anno_dir)\n",
        "  vl_dl = get_dataLoaders(val_dir, val_anno_dir)\n",
        "\n",
        "  if real_dir:\n",
        "    real_dl = get_dataLoaders(real_dir, real_dir)\n",
        "\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "  val_loss = []\n",
        "  val_acc = []\n",
        "  max_acc = 0\n",
        "\n",
        "  print(\"---------------------------------------------------------\")\n",
        "  for epoch in range(epoch_range):\n",
        "    tl = 0\n",
        "    ta = 0\n",
        "    count = 0\n",
        "    for bx, data in enumerate(tqdm(tr_dl, desc = \"TRAINING\")):\n",
        "      loss, acc = train_batch(model, data, optimizer)\n",
        "      tl+=loss\n",
        "      ta+=ta\n",
        "      count+=1\n",
        "    tl = tl/count\n",
        "    ta = ta/count\n",
        "    train_loss.append(tl)\n",
        "    train_acc.append(ta)\n",
        "    \n",
        "    vl = 0\n",
        "    va = 0\n",
        "    count = 0\n",
        "    for bx, data in enumerate(tqdm(vl_dl, desc = \"VALIDATE\")):\n",
        "      loss, acc = valid_batch(model, data)\n",
        "      vl+=loss\n",
        "      va+=acc\n",
        "      count+=1\n",
        "    vl = vl/count\n",
        "    va = va/count\n",
        "    val_loss.append(vl)\n",
        "    val_acc.append(va)\n",
        "\n",
        "\n",
        "    print(\"\\n Epoch: {}/{} | Average Training Loss: {:.4f} | Average Validation Loss: {:.4f} | Validation Accuracy: {:.4f} | Learning Rate: {}\".format(\n",
        "      epoch+1,\n",
        "      epoch_range,\n",
        "      tl, \n",
        "      vl,\n",
        "      va,\n",
        "      1e-4\n",
        "    ))\n",
        "\n",
        "    if va > max_acc:\n",
        "      max_acc = va\n",
        "      torch.save(model.state_dict(),f'Epoch_{epoch+1}_model.pth')\n",
        "      print(\"New Model Saved!\")\n",
        "    \n",
        "    print(\"---------------------------------------------------------\")\n",
        "  save_plot(train_loss, val_loss, \"Training and Validation Loss\")\n",
        "  save_plot(train_acc, val_acc, \"Training and Validation Accuracy\")\n",
        "  \n",
        "  im, mask = next(iter(real_dl))\n",
        "  _mask = model(im)\n",
        "  _, _mask = torch.max(_mask, dim=1)\n",
        "  subplots([im[0].permute(1,2,0).detach().cpu()[:,:,0], \n",
        "                      _mask.permute(1,2,0).detach().cpu()[:,:,0]],nc=2, \n",
        "                      titles=['Original image', \n",
        "                      'Predicted mask'])\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "41TW-a1q7kmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ =='__main__':\n",
        "  run(30, \"/content/drive/MyDrive/Data/dataset1/images_prepped_train\", \n",
        "      \"/content/drive/MyDrive/Data/dataset1/annotations_prepped_train\",\n",
        "      \"/content/drive/MyDrive/Data/dataset1/images_prepped_test\",\n",
        "      \"/content/drive/MyDrive/Data/dataset1/annotations_prepped_test\",\n",
        "      \"/content/drive/MyDrive/Data/dataset1/Test\")"
      ],
      "metadata": {
        "id": "_EpVeWOgEcfq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}