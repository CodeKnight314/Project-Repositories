{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install split-folders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOfmU8QWah0U",
        "outputId": "95fb0a15-2e2e-47d6-bf81-47b86b3991e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import glob\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from splitfolders import splitfolders\n",
        "from torch.optim import Adam\n",
        "from PIL import Image\n",
        "from typing import Tuple, List, Dict\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "#Util functions and calls\n",
        "def show_random_img(directory):\n",
        "  image_list = glob.glob(f'{directory}/*/*.jpg')\n",
        "  return Image.open(random.choice(image_list))\n",
        "\n",
        "def walk_through_directory(directory):\n",
        "  for dir_path, dir_name, files in os.walk(directory):\n",
        "    if(len(files) != 0):\n",
        "      if(files[0][-4:-1] == \".jp\"):\n",
        "        print(f\"There are {len(files)} photos in {dir_path}\")\n",
        "\n",
        "def plot_transformed_images(directory, n,transformation):\n",
        "  image_list = glob.glob(f'{directory}/*/*.jpg')\n",
        "  random_choice = random.sample(image_list, k = n)\n",
        "  for choice in random_choice:\n",
        "    with Image.open(choice) as img:\n",
        "      f, ax = plt.subplots(1,2)\n",
        "      ax[0].imshow(img)\n",
        "      ax[0].set_title(\"Original image\")\n",
        "      ax[0].axis(\"off\")\n",
        "\n",
        "      transformed = transformation(img).permute(1,2,0)\n",
        "      ax[1].imshow(transformed)\n",
        "      ax[1].set_title(\"Transformed image\")\n",
        "      ax[1].axis(\"off\")\n",
        "\n",
        "def split_folder(input_directory, output_directory, train_ratio, test_ratio, validation_ratio):\n",
        "  splitfolders.ratio(input_directory,\n",
        "                     output_directory,\n",
        "                     ratio=(train_ratio,test_ratio,validation_ratio),\n",
        "                     move = True)\n",
        "\n",
        "def test_model_single(model,tr_loader,input_dimension):\n",
        "  img_batch, label_batch = next(iter(tr_loader))\n",
        "\n",
        "  img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
        "  print(f\"Single image shape: {img_single.shape}\\n\")\n",
        "\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    pred = model(img_single.to(device))\n",
        "\n",
        "  print(f\"Output logits:\\n{pred}\\n\")\n",
        "  print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\n",
        "  print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\n",
        "  print(f\"Actual label:\\n{label_single}\")\n",
        "\n",
        "def prediction(model, image_path,class_names):\n",
        "  target_image = torchvision.io.read_image(str(image_path)).type(torch.float64)\n",
        "  target_image = target_image/255\n",
        "  transform = transforms.Compose([transforms.Resize((64,64)),transforms.ToTensor()])\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    target_image = target_image.unsqueeze(dim=0)\n",
        "    target_image_pred = model(target_image.to(device))\n",
        "  \n",
        "  target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
        "\n",
        "  target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
        "  \n",
        "  plt.imshow(target_image.squeeze().permute(1, 2, 0)) \n",
        "  \n",
        "  title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n",
        "\n",
        "  plt.title(title)\n",
        "  plt.axis(False);\n",
        "\n",
        "class CustomDataSet(Dataset):\n",
        "  def __init__(self, dir, transformer=None):\n",
        "    self.paths = list(glob.glob(f'{dir}/*/*.jpg'))\n",
        "    for i in range(len(self.paths)):\n",
        "        self.paths[i] = self.paths[i].replace(\"\\\\\",\"/\")\n",
        "    self.transform = transformer\n",
        "    self.classes, self.class_to_idx = self.find_class(dir)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.paths)\n",
        "  \n",
        "  def __loadimage__(self,index):\n",
        "    img = Image.open(self.paths[index])\n",
        "    return img\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img = self.__loadimage__(index)\n",
        "    path_to_list = self.paths[index].split(\"/\")\n",
        "    class_name = path_to_list[-2]\n",
        "    class_index = self.class_to_idx[class_name]\n",
        "    if self.transform is not None:\n",
        "      tensor = self.transform(img)\n",
        "    else:\n",
        "      tensor = torch.tensor(img)\n",
        "    return tensor.to(device), class_index\n",
        "\n",
        "  def __limit__(self,number):\n",
        "    new_list = []\n",
        "    for classes in self.classes:\n",
        "      count = 0\n",
        "      while(count != number):\n",
        "        random_choice = random.choice(self.paths)\n",
        "        index = self.paths.index(random_choice)\n",
        "        file_class = self.paths[index].split(\"/\")[-2]\n",
        "        if file_class == classes:\n",
        "          count+=1\n",
        "          new_list.append(random_choice)\n",
        "    self.paths = new_list\n",
        "    print(\"adjustment complete\")\n",
        "  \n",
        "  def find_class(self,directory) -> Tuple[List[str], Dict[str, int]]:\n",
        "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
        "    if not classes:\n",
        "      raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n",
        "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
        "    return classes, class_to_idx\n",
        "\n",
        "#Define your network here\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self,input_shape,hidden_units): \n",
        "    super().__init__()\n",
        "    self.conv1 = self.conv_block(input_shape, hidden_units,3,1,1)\n",
        "    self.conv2 = self.conv_block(hidden_units,hidden_units,3,1,1)\n",
        "    self.classifier = self.classification(hidden_units)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.classifier(x)\n",
        "    return x\n",
        "\n",
        "  def conv_block(self,ni,no,kernel_size,stride,padding):\n",
        "    return nn.Sequential(\n",
        "      nn.Conv2d(ni, no, kernel_size, stride,padding),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(no, no, kernel_size, stride,padding),\n",
        "      nn.ReLU(),\n",
        "      nn.BatchNorm2d(no),\n",
        "      nn.MaxPool2d(2)\n",
        "    )\n",
        "\n",
        "  def classification(self,hidden_units):\n",
        "    return nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(hidden_units*16*16,1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "#training related functions\n",
        "def train_batch(x,y,model,optimizer,loss_function):\n",
        "  model.train()\n",
        "  y = y.unsqueeze(-1).float().to(device)\n",
        "  prediction = model(x)\n",
        "  loss_value = loss_function(prediction,y)\n",
        "  loss_value.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  return loss_value.item()\n",
        "\n",
        "def accuracy(x,y,model):\n",
        "  prediction = model(x)\n",
        "  is_correct = (prediction > 0.5) == y.to(device)\n",
        "  return is_correct.cpu().numpy().tolist()\n",
        "\n",
        "def training_process(trn_dl,te_dl,model,optimizer,loss_fn,epoch_range):\n",
        "  train_losses, train_accuracies = [], []\n",
        "  val_accuracies = []\n",
        "  min_training_loss = 100000\n",
        "  for epoch in range(epoch_range):\n",
        "    train_epoch_losses, train_epoch_accuracies = [], []\n",
        "    val_epoch_accuracies = []\n",
        "\n",
        "    for ix, batch in enumerate(tqdm(trn_dl,desc=\"TRAINING LOSS\")):\n",
        "        x, y = batch\n",
        "        batch_loss = train_batch(x, y, model, optimizer, loss_fn)\n",
        "        train_epoch_losses.append(batch_loss) \n",
        "        is_correct = accuracy(x, y, model)\n",
        "        train_epoch_accuracies.extend(is_correct)\n",
        "    train_epoch_loss = np.array(train_epoch_losses).mean()\n",
        "    train_epoch_accuracies = np.array(train_epoch_accuracies).mean()\n",
        "\n",
        "    for ix, batch in enumerate(tqdm(te_dl,desc=\"VALIDATION\")):\n",
        "        x, y = batch\n",
        "        val_is_correct = accuracy(x, y, model)\n",
        "        val_epoch_accuracies.extend(val_is_correct)\n",
        "    val_epoch_accuracy = np.mean(val_epoch_accuracies)\n",
        "\n",
        "    train_losses.append(train_epoch_loss)\n",
        "    train_accuracies.append(train_epoch_accuracy)\n",
        "    val_accuracies.append(val_epoch_accuracy)\n",
        "\n",
        "    print(\"Epoch: {}/{} | Average Loss: {:.4f} | Accuracy: {}\".format(\n",
        "      epoch+1,\n",
        "      epoch_range,\n",
        "      train_epoch_loss,\n",
        "      train_epoch_accuracy\n",
        "    ))\n",
        "\n",
        "    if train_epoch_loss < min_training_loss:\n",
        "      torch.save(model.state_dict(),f'Epoch_{epoch+1}_model.pth')\n",
        "      print(\"New Model Saved!\")\n",
        "      \n",
        "    print(\"---------------------------------------------------------\")\n",
        "  return train_losses, train_accuracies, val_accuracies\n",
        "\n",
        "#parameters\n",
        "if __name__ == '__main__':\n",
        "    batch_size = 16\n",
        "    num_of_workers = os.cpu_count()\n",
        "    test_directory = \"/content/drive/MyDrive/archive/test_set/test_set\"\n",
        "    train_directory = \"/content/drive/MyDrive/archive/training_set/training_set\"\n",
        "\n",
        "    tr_transform = transforms.Compose([\n",
        "        transforms.Resize((64,64)),\n",
        "        transforms.RandomHorizontalFlip(0.2),\n",
        "        transforms.RandomVerticalFlip(0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))\n",
        "    ])\n",
        "\n",
        "    te_transform = transforms.Compose([\n",
        "        transforms.Resize((64,64)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    print(\"Loading Dataset: Training\")\n",
        "    tr_set = CustomDataSet(train_directory, tr_transform)\n",
        "\n",
        "    print(\"Loading Dataset: Testing\")\n",
        "    te_set = CustomDataSet(test_directory, te_transform)\n",
        "\n",
        "    tr_loader = DataLoader(tr_set, batch_size = batch_size, shuffle = True, drop_last = True)\n",
        "    te_loader = DataLoader(te_set, batch_size = batch_size, shuffle = True, drop_last = True)\n",
        "    print(\"Dataloaders established\")\n",
        "\n",
        "    epoch = 10\n",
        "    model = CNN(3,10).to(device)\n",
        "    loss = nn.BCELoss()\n",
        "    optimizer = Adam(model.parameters(),lr = 1e-6)\n",
        "\n",
        "    print(\"Hello World!\")\n",
        "    print(\"Beginning Training\")\n",
        "    training_loss, training_acc, validation_acc = training_process(tr_loader, te_loader, model, optimizer, loss, 20)\n",
        "    plt.plot(training_acc,label = \"\")\n",
        "    plt.plot(validation_acc)"
      ],
      "metadata": {
        "id": "C3_o88xJ8Rgn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3599636-8665-4163-e60c-b40388eaa390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Dataset: Training\n",
            "Loading Dataset: Testing\n"
          ]
        }
      ]
    }
  ]
}