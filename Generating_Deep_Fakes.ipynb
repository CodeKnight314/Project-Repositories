{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ILah7TMVn-P"
      },
      "outputs": [],
      "source": [
        "!wget -q https://www.dropbox.com/s/5ji7jl7httso9ny/person_images.zip\n",
        "!wget -q https://raw.githubusercontent.com/sizhky/deep-fake-util/main/random_warp.py\n",
        "!pip install torch_summary\n",
        "!pip install torch_snippets\n",
        "!mkdir cropped_faces_personA\n",
        "!mkdir cropped_faces_personB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_snippets import *\n",
        "from random_warp import get_training_data\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "XJ2dvp9DV9pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascades_frontalface_default.xml')\n",
        "\n",
        "def crop_face(img):\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  face = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "  if(len(face) > 0):\n",
        "    for (x,y,w,h) in face:\n",
        "        img2 = img[y:(y+h),x:(x+w),:]\n",
        "    img2 = cv2.resize(img2,(256,256))\n",
        "    return img2, True\n",
        "  else:\n",
        "    return img, False\n",
        "\n",
        "def crop_images(folders):\n",
        "  images = Glob(folders + \"/*.jpg\")\n",
        "  for i in range(len(images)):\n",
        "    img = read(images[i], 1)\n",
        "    image, face_detected = crop_face(img)\n",
        "    if face_detected == False:\n",
        "      continue\n",
        "    else:\n",
        "      cv2.imwrite('cropped_faces_'+folders+'/'+str(i)+ '.jpg',cv2.cvtColor(image, cv2.COLOR_RGB2BGR))"
      ],
      "metadata": {
        "id": "x0GP5NECWU4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, personA, personB):\n",
        "    self.itemsA = np.concatenate([read(f,1)[None] for f in personA])/255.\n",
        "    self.itemsB = np.concatenate([read(f,1)[None] for f in personB])/255.\n",
        "    self.itemA += self.itemsB.mean(axis = (0,1,2)) - self.itemsA.mean(axis = (0,1,2))\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.itemsA)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    a, b = choose(self.itemsA), choose(self.itemsB)\n",
        "    return a, b\n",
        "\n",
        "  def collate_fn(self, batch):\n",
        "    a, b = list(zip(*batch))\n",
        "    imsA, target_A = get_training_data(a, len(a))\n",
        "    imsB, target_B = get_training_data(b, len(b))\n",
        "    imsA, target_A, imsB, target_B = [torch.Tensor(i).permute(0,3,1,2).to(device) for i in [imsA, target_A, imsB, target_B]]\n",
        "    return imsA, target_A, imsB, target_B"
      ],
      "metadata": {
        "id": "B7x3qc2wYGzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_layer(input_features, output_features):\n",
        "  return nn.Sequential(\n",
        "      nn.Conv2d(input_features, output_features, kernel_size = 5, stride = 2, padding = 2),\n",
        "      nn.ReLU()\n",
        "  )\n",
        "\n",
        "def up_scale(input_features, output_features):\n",
        "  return nn.Sequential(\n",
        "      nn.ConvTranspose2d(input_features, output_features, kernel_size = 2, stride = 2, padding = 0),\n",
        "      nn.ReLU()\n",
        "  )\n",
        "\n",
        "class Reshape(nn.Module):\n",
        "  def forward(self, input):\n",
        "    output = input.view(-1, 1024, 4, 4)\n",
        "    return output\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.encoder = nn.Sequential(\n",
        "        conv_layer(3,128),\n",
        "        conv_layer(128, 256),\n",
        "        conv_layer(256, 512),\n",
        "        conv_layer(512, 1024),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(1024 * 4 * 4, 1024),\n",
        "        nn.Linear(1024, 1024 * 4 * 4),\n",
        "        Reshape(),\n",
        "        up_scale(1024, 512)\n",
        "    )\n",
        "\n",
        "    self.decoderA = nn.Sequential(\n",
        "        up_scale(512, 256),\n",
        "        up_scale(256, 128),\n",
        "        up_scale(128, 64),\n",
        "        nn.Conv2d(64, 3, kernel_size = 3, padding = 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    self.decoderB = nn.Sequential(\n",
        "        up_scale(512, 256),\n",
        "        up_scale(256, 128),\n",
        "        up_scale(128, 64),\n",
        "        nn.Conv2d(64, 3, kernel_size = 3, padding = 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x, select = \"A\"):\n",
        "    if select == \"A\":\n",
        "      out = self.encoder(x)\n",
        "      out = self.decoderA(out)\n",
        "      return out\n",
        "    else:\n",
        "      out = self.encoder(x)\n",
        "      out = self.decoderB(out)\n",
        "      return out\n"
      ],
      "metadata": {
        "id": "oTKiej7hZslC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_batch(model, data, criterion, optA, optB):\n",
        "  optA, optB = optimizers \n",
        "  optA.zero_grad()\n",
        "  optB.zero_grad()\n",
        "  imsA, target_A, imsB, target_B = data\n",
        "  _imsA = model(imsA)\n",
        "  _imsB = model(imsB)\n",
        "  loss_A = criterion(_imsA, target_A)\n",
        "  loss_B = criterion(_imsB, target_B)\n",
        "  loss_A.backward()\n",
        "  loss_B.backward()\n",
        "  optA.step()\n",
        "  optB.step()\n",
        "\n",
        "  return loss_A.item(), loss_B.item()\n",
        "\n",
        "model = AutoEncoder().to(device)\n",
        "\n",
        "optimizerA = optim.Adam([{'params': model.encoder.parameters()}, {'params': model.decoderA.parameters()}], lr=5e-5, betas=(0.5, 0.999))\n",
        "optimizerB = optim.Adam([{'params': model.encoder.parameters()}, {'params': model.decoderB.parameters()}], lr=5e-5, betas=(0.5, 0.999))\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "n_epochs = 1000\n",
        "log = Report(n_epochs)\n",
        "for ex in range(n_epochs):\n",
        "  N = len(dataloader)\n",
        "  for bx, data in enumerate(tqdm(dataloader)):\n",
        "    lossA, lossB = train_batch(model, data , criterion, optimizerA, optimizerB)\n",
        "    log.record(ex + (1 + bx)/N, lossA = lossA, lossB = lossB, end = '/r')\n",
        "  log.report_avgs(ex+1)\n",
        "  if (ex+1)%100 == 0:\n",
        "      state = {'state': model.state_dict(),'epoch': ex }\n",
        "      torch.save(state, 'autoencoder.pth')\n",
        "  if (ex+1)%100 == 0:\n",
        "      bs = 5\n",
        "      a,b,A,B = data\n",
        "      line('A to B')\n",
        "      _a = model(a[:bs], 'A')\n",
        "      _b = model(a[:bs], 'B')\n",
        "      x = torch.cat([A[:bs],_a,_b])\n",
        "      subplots(x, nc=bs, figsize=(bs*2, 5))\n",
        "      line('B to A')\n",
        "      _a = model(b[:bs], 'A')\n",
        "      _b = model(b[:bs], 'B')\n",
        "      x = torch.cat([B[:bs],_a,_b])\n",
        "      subplots(x, nc=bs, figsize=(bs*2, 5))\n",
        "log.plot_epochs()"
      ],
      "metadata": {
        "id": "P-vmYQTlcG6S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}